<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tae Soo Kim</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/university.shield.small_.blue_.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tae Soo Kim (김태수)</name>
              </p>
              <p>I am a 6th year PhD candidate in the <a href="https://www.cs.jhu.edu/"> Department of Computer Science
                at Johns Hopkins University</a>, where I work on computer vision and machine learning. I work with Prof. <a href="http://www.cs.jhu.edu/~hager/">  Gregory Hager </a> and Prof. <a href="http://www.cs.jhu.edu/~ayuille/">  Alan Yuille.  </a>
              </p>
              <p>
                I develop deep learning models for fine-grained action recognition. My work focuses on few/zero shot
                classification problems with applications in video surveillance and surgical data science. I also develop computer vision
                methods for learning with simulation data.
              </p>
              <p> I received both B.S and M.S.E in computer science from JHU.
<!--                where I worked with Prof. <a href="https://www.cs.jhu.edu/~rht/"> Russell Taylor</a>-->
<!--                and Prof. <a href="https://www.linkedin.com/in/austin-reiter-3962aa7">  Austin Reiter </a> on building real-time 3D reconstruction systems for laparoscopic endoscopic surgery.-->
                I am from Seoul, Korea and I love playing baseball and golf.
                I also served as the 9-th president of the <a href="https://jhukgsa.wixsite.com/kgsa">Korean Graduate Student Association </a> at JHU.

<!--                on computer vision and machine learning.-->
<!--  I am a member of the <a href="https://cirl.lcsr.jhu.edu/"> Computational Interaction and Robotics Laboratory </a>.-->
<!--  My recent work focuses on computer vision methods for structured learning of compositional representation from video data.-->
<!--  I am also interested in using synthetic data to learn complex visual tasks.-->
<!--  I work towards applying such models to activity recognition, skill assessment in surgery and robotics.-->
              </p>

              <p style="text-align:center">
                <a href="data/CurriculumVitae.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BIVP9OsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/TaeSoo-Kim">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/taesookim.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/taesookim.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision and deep learning.
                Much of my research is about activity recognition, surgical data science,
                learning with little to no data, compositional models and learning with simulation data.

              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dazsl.png' width="160">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.03613">
                <papertitle>DASZL: Dynamic Action Signatures for Zero-shot Learning</papertitle>
              </a>
              <br>

              <strong>Tae Soo Kim*</strong>,
              <a href="https://jd-jones.github.io/">Jonathan Jones*</a>,
              <a href="https://scholar.google.com/citations?user=QQhzlS4AAAAJ&hl=en">Michael Peven*</a>,
              <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ&hl=en">Zihao Xiao</a>,
              <a href="https://www.linkedin.com/in/jin-bai-460a51107/">Jin Bai</a>,
              <a href="https://edz-o.github.io/">Yi Zhang</a>,
              <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
              <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>
              <br>
              <em>AAAI</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/1912.03613">arXiv</a> /
              <a href="data/aaai2021_submission.mp4">video</a>
              <p></p>
              <p> This
compositional approach allows us to reframe fine-grained
recognition as zero-shot activity recognition, where a detector is composed “on the fly” from simple first-principles state
                machines supported by deep-learned components.
                Listen to Dr. Alan Yuille talk about this work <a href="https://www.youtube.com/watch?v=iiXeIfnISy4&feature=youtu.be&ab_channel=LVVU2020">here</a> (from 15:00 and on)!

              </p>
            </td>
          </tr>
          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/miccai-w.png' width="160">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/MICCAI-LABELS-2018_CameraReady.pdf">
                <papertitle>Crowdsourcing annotation of surgical instruments in videos of cataract surgery</papertitle>
              </a>
              <br>

              <strong>Tae Soo Kim*</strong>,
              <a href="https://sites.google.com/site/jhuanandmalpani/">Anand Malpani</a>,
              <a href="http://www.cs.jhu.edu/~areiter/JHU/Home.html">Austin Reiter</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>,
              <a href="https://www.hopkinsmedicine.org/profiles/results/directory/profile/1832073/shameema-sikder">Shameema Sikder</a>,
              <a href="https://malonecenter.jhu.edu/people/swaroop-vedula/">Swaroop Vedula</a>

              <br>
              <em>MICCAI-W</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/1912.03613">arXiv</a>
              <p></p>
              <p> We evaluate reliability and validity of crowdsourced annotations for information on surgical
                instruments (name of instruments and pixel location of key points on instruments) in cataract surgery.

              </p>
            </td>
          </tr>
          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/skeletons.png' width="160" height="125">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.03613">
                <papertitle>Interpretable 3D Human Action Analysis with Temporal Convolutional Networks</papertitle>
              </a>
              <br>

              <strong>Tae Soo Kim</strong>,
              <a href="http://www.cs.jhu.edu/~areiter/JHU/Home.html">Austin Reiter</a>
              <br>
              <em>CVPR-W</em>, 2017
              <br>
              <a href="https://arxiv.org/abs/1704.04516">arXiv</a>
              <p></p>
              <p> We re-design the TCN with interpretability in mind and take a step towards a spatio-temporal model that is easier
to understand, explain and interpret.
              </p>
            </td>
          </tr>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/logos.png" width="130" height="130"></td>
            <td width="75%" valign="center">
              Reviewer, CVPR, ICCV, ECCV
              <br><br>
              Reviewer, AAAI
              <br><br>
              Reviewer, MICCAI, IPCAI
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Johns_Hopkins_University's_Academic_Seal.svg.png" alt="jhu" width="130" height="160">
            </td>
            <td width="75%" valign="center">
              Head Teaching Assistant for EN.600.661, Computer Vision. Fall 2015, Fall 2016
              <br>
              <br>
              Head Teaching Assistant for EN.600.684, Augmented Reality. Spring 2016
              <br>
              <br>
              Head Teaching Assistant for EN.600.107, Introductory Programming in Java. Summer 2015
              <br>
              <br>
              Head Teaching Assistant for EN.600.226, Data Structures. Spring 2015
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">

                 <a href="https://jonbarron.info/">Website template credits</a>

                <br>
                Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>

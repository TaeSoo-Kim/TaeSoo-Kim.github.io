<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tae Soo Kim</title>
  
  <meta name="author" content="Tae Soo Kim">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/university.shield.small_.blue_.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tae Soo Kim (김태수)</name>
              </p>
              <p>I am a research scientist and a team lead at <a href="https://www.lunit.io/"> Lunit</a>. I work on research and development of deep learning models for medical image analysis to improve cancer patient care. 
		 Currently, I am leading a team of researchers to recognize various abnormalities from chest X-rays.
	      </p>
	      <p> 
		In summer of 2021, I defended my PhD in Computer Science at Johns Hopkins University where I developed deep learning models for recognizing fine-grained interactions from videos with my primary advisor Prof. <a href="http://www.cs.jhu.edu/~hager/">  Gregory Hager </a>. 
                I also worked closely with Prof. <a href="http://www.cs.jhu.edu/~ayuille/">  Alan Yuille </a> and <a href="https://www.linkedin.com/in/austin-reiter-3962aa7">  Austin Reiter. </a>
	      </p>

              <p> I received both B.S and M.S.E in computer science from JHU.
<!--                where I worked with Prof. <a href="https://www.cs.jhu.edu/~rht/"> Russell Taylor</a>-->
<!--                and Prof. <a href="https://www.linkedin.com/in/austin-reiter-3962aa7">  Austin Reiter </a> on building real-time 3D reconstruction systems for laparoscopic endoscopic surgery.-->
                I am from Seoul, Korea and I enjoy playing golf.
                I also served as the 9-th president of the <a href="https://jhukgsa.wixsite.com/kgsa">Korean Graduate Student Association </a> at JHU.
	      </p>
		
<!--                on computer vision and machine learning.-->
<!--  I am a member of the <a href="https://cirl.lcsr.jhu.edu/"> Computational Interaction and Robotics Laboratory </a>.-->
<!--  My recent work focuses on computer vision methods for structured learning of compositional representation from video data.-->
<!--  I am also interested in using synthetic data to learn complex visual tasks.-->
<!--  I work towards applying such models to activity recognition, skill assessment in surgery and robotics.-->
              

              <p style="text-align:center">
                <a href="data/Curriculum_Vitae.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BIVP9OsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/TaeSoo-Kim">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/taesookim.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/taesookim.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
		  <li><strong>2022-05</strong>: One paper "Video-based assessment of intraoperative surgical skill" accepted to IJCARS 2022. </li>
		  <li><strong>2022-02</strong>: Our MICCAI tutorial on <a href="https://sites.google.com/lunit.io/miccai22-tutorial-practical-ai/home">AI for medical image analysis in practice</a> is accepted. See you at MICCAI! </li>
		  <li><strong>2021-11</strong>: One workshop paper "Learning from synthetic vehicles" accepted to WACV-2022 RWS workshop. </li>
		  <li><strong>2021-08</strong>: One paper "Motion Guided Attention Fusion to recognize interactions from videos" accepted to ICCV-2021. </li>
		  <li><strong>2021-06</strong>: I joined <a href="https://www.lunit.io/"> Lunit Inc</a>. as a full-time research scientist!</li>
		  <li><strong>2021-05</strong>: I successfully defended my PhD thesis "Model-driven and Data-driven Methods for Recognizing Compositional Interactions from Videos".</li>
		</ul>

              </p>
            </td>
          </tr>
        </tbody></table>
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision and deep learning. 
		Much of my research is about fine-grained recognition and learning with little to no labeled data.
		Currently, I care mostly about self-supervised learning, custimization and generalization of fine-grained recognition models.  .

              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	
	<tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ijcars.png' width="160" height="130">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.06416.pdf">
                <papertitle>Video-based assessment of intraoperative surgical skill</papertitle>
              </a>
              <br>

              <a href="">Sanchit Hira</a>,
              <a href="">Digvijay Singh</a>,
	      <strong>Tae Soo Kim</strong>,
              <a href="">Shobhit Gupta</a>,
	      <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>,
              <a href="https://www.hopkinsmedicine.org/profiles/results/directory/profile/1832073/shameema-sikder">Shameema Sikder</a>,
	     <a href="https://malonecenter.jhu.edu/people/swaroop-vedula/">Swaroop Vedula</a>,
		    
              
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>
              <br>
              <em>IJCARS</em>, 2022
              <br>
              <p></p>
              <p> Can deep learning models assess the quality of a surgery directly from a video? We show that our video analysis model can accurately assess surgical skill from real world cataract surgeries. 
              </p>
            </td>
          </tr>
		
	<tr onmouseout="inerf_stop()" onmouseover="inerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='inerf_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/icip_2021_video.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/icip_thumbnail.png' width="160" >
              </div>
              <script type="text/javascript">
                function inerf_start() {
                  document.getElementById('inerf_image').style.opacity = "1";
                }
                function inerf_stop() {
                  document.getElementById('inerf_image').style.opacity = "0";
                }
                inerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/SyntheticVehicles_ICIP2021.pdf">
                <papertitle>Learning from synthetic vehicles</papertitle>
              </a>
              <br>
              <strong>Tae Soo Kim</strong>,
              <a href="https://www.linkedin.com/in/bohoon-shim-83a61417a/">Bohoon Shim</a>,
              <a href="https://scholar.google.com/citations?user=QQhzlS4AAAAJ&hl=en">Michael Peven</a>,
              <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
              <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>
              <br>
              <em>WACV-RWS</em>, 2022 /
              <a href="https://archive.org/details/saved-v-1">Dataset publicly available here</a>
              <br>
              <p></p>
              <p>Simulated Articulated VEhicles Dataset (SAVED) is the first dataset of synthetic vehicles with moveable parts. Using SAVED, we show that we can train a model with synthetic images to recognize fine-grained vehicle parts and orientation directly from real images.
              </p>
            </td>
          </tr> 
		
		
           <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/iccv21_thumbnail.png' width="160" >
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/iccv2021_withnames.pdf">
                <papertitle>Motion Guided Attention Fusion to recognize interactions from videos</papertitle>
              </a>
              <br>

              <strong>Tae Soo Kim</strong>,
              <a href="https://jd-jones.github.io/">Jonathan Jones</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>
              <br>
              <em>ICCV</em>, 2021 
              <br>
              <p></p>
              <p> 
	      Do current video models have the ability to recognize an unseen instantiation of an interaction defined using a combination of seen components?  We show that it ispossible by specifying the dynamic structure of an action using a sequence of object detections in a top-down fashion. When the top-down structure is combined with a dual-pathway bottom-up approach, we show that the model can then generalize even to unseen interactions.
              </p>
            </td>
	


          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dazsl.png' width="160">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.03613">
                <papertitle>DASZL: Dynamic Action Signatures for Zero-shot Learning</papertitle>
              </a>
              <br>

              <strong>Tae Soo Kim*</strong>,
              <a href="https://jd-jones.github.io/">Jonathan Jones*</a>,
              <a href="https://scholar.google.com/citations?user=QQhzlS4AAAAJ&hl=en">Michael Peven*</a>,
              <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ&hl=en">Zihao Xiao</a>,
              <a href="https://www.linkedin.com/in/jin-bai-460a51107/">Jin Bai</a>,
              <a href="https://edz-o.github.io/">Yi Zhang</a>,
              <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
              <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>
              <br>
              <em>AAAI</em>, 2021
              <br>
              <a href="data/aaai2021_submission.mp4">video</a>
              <p></p>
              <p> This
compositional approach allows us to reframe fine-grained
recognition as zero-shot activity recognition, where a detector is composed “on the fly” from simple first-principles state
                machines supported by deep-learned components.
                Listen to Dr. Alan Yuille talk about this work <a href="https://www.youtube.com/watch?v=iiXeIfnISy4&feature=youtu.be&ab_channel=LVVU2020">here</a> (from 15:00 and on)!

              </p>
            </td>
          </tr>
          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/safer.png' width="160" height="130">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/SAFER_with_names.pdf">
                <papertitle>SAFER: Fine-grained activity detection by compositional hypothesis testing</papertitle>
              </a>
              <br>

              <strong>Tae Soo Kim</strong>,
              <a href="https://edz-o.github.io/">Yi Zhang</a>,
              <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ&hl=en">Zihao Xiao</a>,
              <a href="https://scholar.google.com/citations?user=QQhzlS4AAAAJ&hl=en">Michael Peven</a>,
              <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
              <a href="https://www.linkedin.com/in/jin-bai-460a51107/">Jin Bai</a>,
              <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>
              <br>
              <em>arxiv</em>, 2019
              <br>
              <p></p>
              <p> SAFER models a large space of fine-grained activities using a small set
of detectable entities and their interactions.
Such a design scales effectively with concurrent developments of object detectors, parsers and more.
                Our model effectively detects fine-grained human activities without any activity level supervision in video surveillance applications.
              </p>
            </td>
          </tr>

          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/wacv.png' width="160">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/wacv_ready.pdf">
                <papertitle>Synthesizing attributes with unreal engine for fine-grained activity analysis</papertitle>
              </a>
              <br>

              <strong>Tae Soo Kim</strong>,
              <a href="https://scholar.google.com/citations?user=QQhzlS4AAAAJ&hl=en">Michael Peven</a>,
              <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
              <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>
              <br>
              <em>WACV-W</em>, 2019
              <br>
              <p></p>
              <p> Recent deep neural network based computer vision models can be trained to recognize pretty much anything given enough data. We show we can synthesize visual attributes using the UnrealEngine4 to train activity classification models.
              </p>
            </td>
          </tr>
          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/jama.png' width="160">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2729808">
                <papertitle>Assessment of automated identification of phases in videos of cataract surgery using machine learning and deep learning techniques</papertitle>
              </a>
              <br>
              <strong>Tae Soo Kim*</strong>,
              <a href="https://www.cs.princeton.edu/~felixy/">Felix Yu*</a>,
              <a href="">Gianluca Silva Croso*</a>,
              <a href="https://www.linkedin.com/in/ziang-song-43b0ab8a/">Ziang Song</a>,
              <a href="https://felixparker.com/">Felix Parker</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>,
              <a href="http://www.cs.jhu.edu/~areiter/JHU/Home.html">Austin Reiter</a>,
              <a href="https://malonecenter.jhu.edu/people/swaroop-vedula/">Swaroop Vedula</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=FGvEN9QAAAAJ&view_op=list_works&sortby=pubdate">Haider Ali</a>,
              <a href="https://www.hopkinsmedicine.org/profiles/results/directory/profile/1832073/shameema-sikder">Shameema Sikder</a>


              <br>
              <em>JAMA Network Open</em>, 2019
              <br>

              <p></p>
              <p>  Competence in cataract surgery is a public health necessity, and videos of cataract surgery are routinely available to educators and trainees but currently are of limited use in training.
                We develop tools that efficiently segment videos of cataract surgery into constituent phases for subsequent automated skill assessment and feedback.
              </p>
            </td>
          </tr>
          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/ipcai.png' width="160">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/IJCARS_FINAL.pdf">
                <papertitle>Objective assessment of intraoperative technical skill in capsulorhexis using videos of cataract surgery</papertitle>
              </a>
              <br>
              <strong>Tae Soo Kim</strong>,
              <a href="http://www.cs.jhu.edu/~mollyob/">Molly O'Brien</a>,
              <a href="https://www.linkedin.com/in/sidra-zafar-45a8b0115/?originalSubdomain=pk">Sidra Zafar</a>,
              <a href="https://sites.google.com/site/jhuanandmalpani/">Anand Malpani</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>,
              <a href="https://www.hopkinsmedicine.org/profiles/results/directory/profile/1832073/shameema-sikder">Shameema Sikder</a>,
              <a href="https://malonecenter.jhu.edu/people/swaroop-vedula/">Swaroop Vedula</a>

              <br>
              <em>IJCARS</em>, 2019
              <br>

              <p></p>
              <p> We introduce a model for objective assessment of surgical skill from videos of microscopic cataract surgery.
                Our model can accurately predict surgeon's skill level from tool tip movements captured in the surgical view.

              </p>
            </td>
          </tr>
          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/miccai-w.png' width="160">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/MICCAI-LABELS-2018_CameraReady.pdf">
                <papertitle>Crowdsourcing annotation of surgical instruments in videos of cataract surgery</papertitle>
              </a>
              <br>

              <strong>Tae Soo Kim</strong>,
              <a href="https://sites.google.com/site/jhuanandmalpani/">Anand Malpani</a>,
              <a href="http://www.cs.jhu.edu/~areiter/JHU/Home.html">Austin Reiter</a>,
              <a href="http://www.cs.jhu.edu/~hager/">Gregory D. Hager</a>,
              <a href="https://www.hopkinsmedicine.org/profiles/results/directory/profile/1832073/shameema-sikder">Shameema Sikder</a>,
              <a href="https://malonecenter.jhu.edu/people/swaroop-vedula/">Swaroop Vedula</a>

              <br>
              <em>MICCAI-W</em>, 2018
              <br>

              <p></p>
              <p> We evaluate reliability and validity of crowdsourced annotations for information on surgical
                instruments (name of instruments and pixel location of key points on instruments) in cataract surgery.

              </p>
            </td>
          </tr>
          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/unrealcv.png' width="160">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/unrealcv.pdf">
                <papertitle>UnrealCV: Virtual Worlds for Computer Vision</papertitle>
              </a>
              <br>
              <a href="https://weichaoqiu.com/">Weichao Qiu</a>,
              <a href="https://fangweizhong.xyz/">Fangwei Zhong</a>,
              <a href="https://edz-o.github.io/">Yi Zhang</a>,
              <a href="https://www.cs.jhu.edu/~syqiao/">Siyuan Qiao</a>,
              <a href="https://scholar.google.com/citations?user=ucb6UssAAAAJ&hl=en">Zihao Xiao</a>,
              <strong>Tae Soo Kim</strong>,
              <a href="https://cfcs.pku.edu.cn/english/people/faculty/yizhouwang/index.htm">Yizhou Wang</a>,
              <a href="http://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>
              <br>
              <em>ACM-MM</em>, 2017
              <br>
              <p></p>
              <p> UnrealCV is an open source project to help computer vision researchers build virtual worlds using Unreal Engine 4 (UE4).
                  Check it out <a href="https://unrealcv.org/">here</a>.

              </p>
            </td>
          </tr>
          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/skeletons.png' width="160" height="125">
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1704.04516">
                <papertitle>Interpretable 3D Human Action Analysis with Temporal Convolutional Networks</papertitle>
              </a>
              <br>

              <strong>Tae Soo Kim</strong>,
              <a href="http://www.cs.jhu.edu/~areiter/JHU/Home.html">Austin Reiter</a>
              <br>
              <em>CVPR-W</em>, 2017
              <br>
              <p></p>
              <p> We re-design the TCN with interpretability in mind and take a step towards a spatio-temporal model that is easier
to understand, explain and interpret.
              </p>
            </td>
          </tr>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/logos.png" width="130" height="130"></td>
            <td width="75%" valign="center">
              Reviewer, CVPR, ICCV, ECCV
              <br><br>
              Reviewer, AAAI
              <br><br>
              Reviewer, MICCAI, IPCAI
            </td>
          </tr>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Johns_Hopkins_University's_Academic_Seal.svg.png" alt="jhu" width="130" height="160">
            </td>
            <td width="75%" valign="center">
              Head Teaching Assistant for EN.600.661, Computer Vision. Fall 2015, Fall 2016
              <br>
              <br>
              Head Teaching Assistant for EN.600.684, Augmented Reality. Spring 2016
              <br>
              <br>
              Head Teaching Assistant for EN.600.107, Introductory Programming in Java. Summer 2015
              <br>
              <br>
              Head Teaching Assistant for EN.600.226, Data Structures. Spring 2015
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">

                 <a href="https://jonbarron.info/">Website template credits</a>

                <br>
                Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
